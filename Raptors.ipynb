{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dBYEhyhR_EC7"},"outputs":[],"source":["!git clone https://github.com/parthsarthi03/raptor.git\n","%cd raptor\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPsRBFXpABVV"},"outputs":[],"source":["import os\n","os.environ[\"OPENAI_API_KEY\"] = \"my key\"\n","\n","from raptor import RetrievalAugmentation\n","\n","# Initialize with default configuration. For advanced configurations, check the documentation. [WIP]\n","RA = RetrievalAugmentation()"]},{"cell_type":"code","source":["! pip install pdfplumber"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCn34994Lej6","executionInfo":{"status":"ok","timestamp":1723728969890,"user_tz":-480,"elapsed":3441,"user":{"displayName":"Jacob Wood","userId":"06964700642539016767"}},"outputId":"f743d9f6-aeff-4004-b1b7-c8ce186be004"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.3)\n","Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n","Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n","Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.8)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"]}]},{"cell_type":"code","source":["import pdfplumber\n","\n","pdf_path = \"/content/raptor/baichuan.pdf\"\n","\n","def extract_text_from_pdf(pdf_path):\n","    text = \"\"\n","    with pdfplumber.open(pdf_path) as pdf:\n","        for page in pdf.pages:\n","            text += page.extract_text()\n","    return text\n","\n","text = extract_text_from_pdf(pdf_path)\n"],"metadata":{"id":"Cbf66gOHqaI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-rStBMuAuWe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723729366871,"user_tz":-480,"elapsed":250620,"user":{"displayName":"Jacob Wood","userId":"06964700642539016767"}},"outputId":"4c4a2c5b-6eb0-490e-9c83-3f69287cba2c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7cccf5820670>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n","    self._make_controller_from_path(filepath)\n","  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 1175, in _make_controller_from_path\n","    lib_controller = controller_class(\n","  File \"/usr/local/lib/python3.10/dist-packages/threadpoolctl.py\", line 114, in __init__\n","    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n","  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n","    self._handle = _dlopen(self._name, mode)\n","OSError: dlopen() error\n"]}],"source":["RA.add_documents(text)"]},{"cell_type":"code","source":["question = \"What is the vocabulary size of Baichuan 2?\"\n","\n","answer = RA.answer_question(question=question)\n","\n","print(\"Answer: \", answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FRryFHfsReU","executionInfo":{"status":"ok","timestamp":1723729536072,"user_tz":-480,"elapsed":3291,"user":{"displayName":"Jacob Wood","userId":"06964700642539016767"}},"outputId":"9e79a5fb-843f-4519-bdc0-3bc06202d8db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer:  The vocabulary size of Baichuan 2 is 125,696, which has been expanded from 64,000 in Baichuan 1 to strike a balance between computational efficiency and model performance.\n"]}]},{"cell_type":"markdown","source":["问题及答案\n","\n","1.What are the two main configurations of Baichuan 2 models?\n","\n","Baichuan 2 models come in two main configurations: 7 billion parameters and 13 billion parameters.\n","\n","2.How many tokens were used to train the Baichuan 2 models?\n","\n","The models were trained on 2.6 trillion tokens.\n","\n","3.In which areas does Baichuan 2 outperform other open-source models?\n","\n","Baichuan 2 outperforms in math, code, medical, and legal tasks.\n","\n","4.What is the purpose of the chat models included in Baichuan 2?\n","\n","The chat models are optimized for human interaction.\n","\n","5.How does Baichuan 2 contribute to the AI community?\n","\n","Baichuan 2's models and training checkpoints are open-sourced to support further research and development.\n","\n","6.What is the vocabulary size of Baichuan 2?\n","\n","The vocabulary size is 100,000 tokens.\n","\n","7.What benchmarks were used to evaluate Baichuan 2's performance?\n","\n","Specific benchmarks were not detailed in the summary, but it was mentioned that Baichuan 2 performs well across various tasks.\n","\n","8.What are some specific tasks Baichuan 2 excels in?\n","\n","Baichuan 2 excels in tasks related to math, code, medical, and legal fields.\n","\n","9.What is the significance of open-sourcing Baichuan 2's models and training checkpoints?\n","\n","Open-sourcing allows the AI community to use, study, and build upon Baichuan 2, fostering further innovation and research.\n","\n","10.How does Baichuan 2 handle multilingual tasks?\n","\n","Baichuan 2 is designed as a multilingual language model, capable of performing tasks in multiple languages"],"metadata":{"id":"sai3hP66OH-T"}},{"cell_type":"markdown","source":["# 使用raptor后的回答："],"metadata":{"id":"khHSFzLYgi99"}},{"cell_type":"markdown","source":["1.The two main configurations of Baichuan 2 models are Baichuan 2-7B with 7 billion parameters and Baichuan 2-13B with 13 billion parameters. These models have been designed to excel in vertical domains such as medicine and law, showcasing significant advancements over their predecessor, Baichuan 1, particularly in performance on math and code problems. The Baichuan 2-7B model has achieved a Test Accuracy of 54.5%, while the Baichuan 2-13B model has achieved a Test Accuracy of 61%.\n","\n","2.The Baichuan 2 models were trained on 2.6 trillion tokens.\n","\n","3.Baichuan 2 outperforms other open-source models in various areas, including math and code problem-solving tasks. It surpasses its predecessor, Baichuan 1, by nearly 30% on benchmarks like MMLU, CMMLU, and C-Eval. Additionally, Baichuan 2 is optimized to enhance performance in domains such as medical and legal tasks, as well as on specific datasets like MedQA. The model's foundation models exhibit significant improvements compared to other open or closed-source models, making Baichuan 2 a suitable choice for domain-specific optimization and achieving superior overall performance in tasks like zh-ar, zh-ru, and zh-ja.\n","\n","4.The purpose of the chat models included in Baichuan 2 is to excel in dialogue and context understanding, optimize to follow human instructions, and showcase scaling results. These chat models, such as Baichuan 2-7B-Chat and Baichuan 2-13B-Chat, are designed to generate responses based on prompts and demonstrate excellence in dialogue and context understanding. Additionally, efforts are made to improve the safety of these chat models through supervised fine-tuning and reinforcement learning from human feedback.\n","\n","5.Baichuan 2 contributes to the AI community by introducing a series of large-scale multilingual language models that significantly outperform its predecessor, Baichuan 1. It consists of two separate models - Baichuan 2-7B with 7 billion parameters and Baichuan 2-13B with 13 billion parameters. The model is optimized to enhance performance in specialized fields such as medicine, law, mathematics, and coding. Baichuan 2 also emphasizes transparency in its development process, sharing insights into trials, errors, and lessons learned. By providing access to advanced language models and promoting openness and transparency, Baichuan 2 aims to advance collective knowledge in developing language models and benefit the research community\n"],"metadata":{"id":"2idJDvplg3bF"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkfdPiWFriZ5qpJcHdfPKJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}